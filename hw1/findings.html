<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Findings</title>
    <link href="styles.css" rel="stylesheet">
</head>

<body>
<header>
    <h1>Findings</h1>
    <nav>
    <a href="homepage.html">Home</a>
    <a href="findings.html">Findings</a>
    <a href="analysis.html">Analysis</a>
    <a href="conclusions.html">Conclusions</a>
    </nav>
</header>

<br>


<article>
    <h2>Findings of the hallucinations</h2>
<p>Recent studies, including one released by OpenAI, 
    have highlighted several reasons why language models hallucinate. 
    First, the data they are trained on often contains errors. When asked about rare or uncertain topics (such as the user's birthday or the time currently), 
    models may fill in the blanks with invented details. Second, the way models are rewarded during training encourages them to sound confident, 
    since a fluent but wrong answer is scored better than no answer at all. The methods used to decode answers, such as predicting the most likely next word-push models
    to produce something rather than remain uncertain, similar to how autocorrect on your phone suggests or completes a word even if it's not always correct. </p>
    <br>
 </article>   

<figure>
<img id="metrics" src="https://i.imgur.com/eJp8vta.png" referrerpolicy="no-referrer" alt="Metrics">
<figcaption>OpenAI metric for trade-offs between abstaining, answer correctly, and incorrectly.</figcaption>
</figure>

<br>
<article>
<p>These factors together explain why models can be “confidently wrong.” They are not deliberately lying, 
    but they are trained in a way that favors plausible-sounding answers over cautious ones. 
    Understanding this behavior is important for building safer AI systems.</p>
    </article>


</body>
</html>