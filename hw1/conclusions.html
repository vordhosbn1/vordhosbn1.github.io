<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Conclusions</title>
    <link href="styles.css" rel="stylesheet">
</head>

<body>
<header>
    <h1>Conclusion</h1>
    <nav>
    <a href="homepage.html">Home</a>
    <a href="findings.html">Findings</a>
    <a href="analysis.html">Analysis</a>
    <a href="conclusions.html">Conclusions</a>
    </nav>
    
</header>
    <article>
    <p>
    Hallucinations in language models remain one of the most difficult challenges in AI research 
    and are not caused by a single bug, but by how models are trained, rewarded, and used in practice. 
    As a result, they can appear in simple conversations or in high-stakes settings like code generation and data management (Fortune article)
    </p>
    </article>
    

    <article>
    <p>
    Possible ways forward include training models to express uncertainty, 
    creating safeguards that limit the damage a hallucination can cause, and encouraging users to double-check AI outputs instead of blindly trusting them. 
    Equally important are basic practices like frequent backups and human review. While progress has been made, specifically with OpenAI's new model, ChatGPT 5, 
    hallucinations show that even advanced AI systems should be treated as fallible tools rather than perfect sources of truth. Another area that is worth considering
    is how hallucinations manifest across different languages. For example, English, with  its ambiguity and reliance on context, may encourage models to generate
    more "confidently wrong" completions. In contrast, languages that are less ambiguous like Chinese, often encode in their own language, its semantic meaning more 
    directly within its characters, which can potentially reduce errors. Research in multilingual systems such as DeepSeek may help reveal whether hallucinations are common
    across languages, or if some linguistic structure offers more (suggesting a language built specifically for LLMs). This line of inquiry points to the fact that hallucinations
    are not only a technical issue, but a linguistic one that by addressing them, may require attention to how models learn and represent meaning.
    </p>
    </article>

    
</body>
</html>